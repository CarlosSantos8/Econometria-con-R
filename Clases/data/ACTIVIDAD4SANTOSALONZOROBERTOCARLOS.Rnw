\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish, activeacute, mexico]{babel}
\usepackage[T1]{fontenc}
\usepackage{pdfsync}
\usepackage[nottoc]{tocbibind}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2.00cm, right=2.00cm, top=1.50cm, bottom=2.00cm]{geometry}
\author{SANTOS ALONZO ROBERTO CARLOS}
\title{ACTIVIDAD}
\begin{document}
\begin{titlepage}
\centering
{\bfseries\LARGE Escuela Superior de Física y Matemáticas \par}

\vspace{3cm}
{\scshape\Huge ECONOMETRÍA \par}
\vspace{3cm}
 
\vfill
{\Large Alumno: \par}
{\Large Roberto Carlos Santos Alonzo \par}
\vfill
{\Large Actividad 4 \par}
\vfill

{\Large Diciembre 2020 \par}
\end{titlepage}
\newpage
\def\contentsname{Ejercicios}
{\tableofcontents}
\maketitle
<<eval=TRUE, echo=TRUE, fig.height=3.5, fig.width=4.5, message=FALSE, warning=FALSE, comment=NA>>=
library(lmtest)
library(sandwich)
library(car)
@

\section{La función de demanda de un bien depende del precio ($p_t$) y el ingreso ($y_t$)}
\begin{align*}
q_t&=\beta _0+\beta _1p_t+\beta _2y_t+u_t
\end{align*}
Sin embargo, un investigador omite la variable ingreso y estima el modelo
\begin{align*}
 q_t&=\beta _0+\beta _1p_t+u_t
\end{align*}
demostrar que si $cov(p_t,y_t)\neq 0$, el estimador $\beta _1$ es sesgado.

\textbf{Demostración:} \\ Tenemos que:
\begin{align*}
q_t - \bar{q}&= \beta _1(p_t - \bar{q}) + u_t - \bar{u}\\
\sum_{}^{}(p_t - \bar{p})(q_t - \bar{q}) &= \sum_{}^{}(p_t - \bar{p})[\beta_1(p_t - \bar{p}) + u_t - \bar{u}] = \beta_1\sum_{}^{}(p_t - \bar{p})^2 + \sum_{}^{}(p_t - \bar{p})(u_t - \bar{u}) 
\end{align*}
De lo anterior
\begin{align*}
\sum (p_t - \bar{p})(u_t -\bar{u}) &= \sum (p_t - \bar{p})u_t - \sum (p_t - \bar{p})\bar{u} = \sum (p_t - \bar{p})u_t \\ \longrightarrow &\sum (p_t - \bar{p})\bar{u} = 0 \\  \sum_{}^{}(p_t - \bar{p})(q_t - \bar{q}) &= \beta_1\sum (p_t - \bar{p})^2 + \sum (p_t - \bar{p})u_t
\end{align*}
 Ahora, tenemos que:
 \begin{align*}
 cov(p_t,y_t) & \neq 0 \\  \hat{\beta }&= \frac{\sum (p_t - \bar{p})(q_t - \bar{q})}{\sum (p_t - \bar{p})^2} = \frac{\beta_1\sum (p_t - \bar{p})^2 + \sum (p_t - \bar{p})u_t^\ast }{\sum (p_t - \bar{p})^2} \\ \textit{Con } \\ u_t^\ast &= u_t + \sum_{}^{}(y_t - \bar{y})
 \end{align*}
 Desarrollando,\\
 \begin{align*}
 \hat{\beta } = \beta_1 + \frac{\sum_{}^{}(p_t - \bar{p})u_t}{\sum_{}^{}(p_t - \bar{p})^2} + \frac{\sum_{}^{}(p_t - \bar{p})(y_t - \bar{y})}{\sum_{}^{}(p_t - \bar{p})^2}
 \end{align*} \\
 Ahora, calculando el valor esperado de $\hat{\beta}$ obtenemos,
 
 \begin{align*}
  E[\hat{\beta}] = E[\beta_1] + E[\frac{\sum_{}^{}(p_t - \bar{p})u_t}{\sum_{}^{}(p_t - \bar{p})^2}] + E[\frac{\sum_{}^{}(p_t - \bar{p})(y_t - \bar{y})}{\sum_{}^{}(p_t - \bar{p})^2}]
 \end{align*}
donde 

 \begin{align*}
  E[\frac{\frac{\sum (p_t - \bar{p})u_t}{n}}{\frac{\sum (p_t - \bar{p})^2}{n}}] &= 0 &  E[\frac{\frac{\sum (p_t - \bar{p})(y_t - \bar{y})}{n}}{\frac{\sum (p_t - \bar{p})^2}{n}}] &\neq 0
 \end{align*} \\
 \begin{align*}
 \longrightarrow E[\hat{\beta}] &\neq E[\beta_1] \\ \therefore &\beta _1\textbf{ Es sesgado}
 \end{align*}





\section{Considerar el modelo de regresión lineal }
\begin{align*}
y_i&=\alpha +\beta x_i+u_i \\ \textit{suponer que $c_1$ y $c_2$ constantes con $c_2\neq 0$ tal que} \\ c_iy_i&=\alpha+\beta c_2x_i+u_i
\end{align*}
demostrar que 
\begin{align*}
\alpha ^\ast &=c_1\hat{\alpha} & \beta ^\ast &=\frac{c_1}{c_2}\hat{\beta }
\end{align*}
\textbf{Demostración:}
Sabemos que
\begin{align*}
\hat{\alpha}&= \bar{Y} - \hat{\beta}\bar{X} \\ \hat{\beta } &= \frac{\sum (x_i - \bar{X})(y_i - \bar{Y})}{\sum (x_i - \bar{X})^2}
\end{align*}
Sean
\begin{align*}
Y_i^\ast &= c_1Y_i &  X_i^\ast &= c_2X_i
\end{align*}
entonces 
\begin{align*}
  \bar {Y}^\ast &= \frac{\sum (Y_i^\ast )}{n} \\ \bar {X^\ast } &= \frac{\sum (X_i^\ast )}{n} 
  \end{align*}

Sustituyendo, tenemos que:
\begin{align*}
  \beta^\ast &= \frac{\sum_{}^{}(c_2x_i - c_2\bar{X})(c_1y_i - c_1\bar{Y})}{\sum (c_2x_i - c_2\bar{X})^2} = c_1c_2\frac{\sum (x_i - \bar{X})(y_i - \bar{Y})}{\sum c_2^2(x_i - \bar{X})^2}=\frac{c_1}{c_2}\frac{\sum (x_i - \bar{X})(y_i - \bar{Y})}{\sum (x_i - \bar{X})^2} \\ \therefore \beta ^\ast &= \frac{c_1}{c_2}\hat{\beta}
\end{align*} \\ \\
Ahora, 
\begin{align*}
\alpha &= \bar{Y}-\beta \bar{X}\\
 \alpha^\ast &= \bar{Y^\ast }-\beta^\ast \bar{X^\ast } \\ \textit{Sustituyendo } \\ \alpha^\ast &= c_1\bar{Y} - \frac{c_1}{c_2}\hat{\beta}c_2\bar{X} = c_1(\bar{Y} - \hat{\beta}\bar{X}) \\ \therefore \alpha^\ast &= c_1\hat{\alpha}
\end{align*} 











\section{Comprobar que: }
\begin{align*}
F=\frac{\frac{SRC_r - SRC_{nr}}{q}}{\frac{SRC_{nr}}{n-k}}=\frac{\frac{R^2_{nr} - R^2_r}{q}}{\frac{1-R^2_{nr}}{n-k}}
\end{align*}

\textbf{Solución: } \\
\begin{align*}
\textit{Por definición tenemos } \\
R^2&=1-\frac{SRC}{STC} \\ \textit{Entonces, $R^2_{nr}$ no restringida es:} \\ R^2_{nr}&=1-\frac{SRC_{nr}}{STC}\\ 1-R^2_{nr}&=\frac{SRC_{nr} }{STC} \\ \textit{$R^2_r$ restringida es:} \\ R^2_r&=1-\frac{SRC_r }{STC} \\ \textit{Sustituyendo, tenemos que:} \\ F&=\frac{\frac{R^2_{nr} - R^2_r}{q}}{\frac{1-R^2_{nr}}{n-k}}\\ F&=\frac{\frac{\frac{SRC_r }{STC} -\frac{SRC_{nr} }{STC}}{q}}{\frac{1-\frac{SRC_{nr} }{STC}}{n-k}} \\ \therefore F&=\frac{\frac{SRC_r - SRC_{nr}}{q}}{\frac{SRC_{nr}}{n-k}}
\end{align*}







\section{Sea el modelo de regresión lineal}
$$y_i=\beta _0+\beta _1x_{xi}+\beta _2x_{i2}+\beta _3x_{i3}+u_i$$ donde $y$ se distribuye normalmente con media $\mu =2$ y varianza unitaria $\sigma =1$, $x_i$ como una binomial con probabilidad $p=0.5$ y $x=5$, $x_2$ exponencial con $\lambda =1$ y $x_3$ como Poisson con media . El tamaño de muestra es $n=200$
<<>>=
y <-c(rnorm(200,2,1))
x.1 <-c(rbinom(200,5,0.5))
x.2 <-c(rexp(200,1))
x.3 <-c(rpois(200,4))
@

<<>>=
datos <- data.frame(y,x.1,x.2,x.3)
@

\subsection{Calcular residuales ($\hat{e}_x$)de la regresión de $x_{i1}$ sobre $x_{i2}$ y $x_{i2}$ }
<<>>=
ols.x <- lm(x.1 ~ x.2 + x.3)
e.x <- residuals(ols.x)^2;e.x
@

\subsection{Calcular residuales ($\hat{e}_y$)de la regresión de $y_{i}$ sobre $x_{i2}$ y $x_{i2}$}
<<>>=
ols.y <- lm(y ~ x.2 + x.3)
e.y <- residuals(ols.y)^2;e.y
@

\subsection{Calcular la regresión de $\hat{e}_y$ contra $\hat{e}_x$}
<<>>=
ols.yx <- lm(e.y ~ e.x);ols.yx
@








\section{El siguiente modelo explica el peso de un niño al nacer en términos de distintos factores.}
\begin{align*}
  bwght_i = \beta_0 + \beta_1cigs_i + \beta_2parity_i + \beta_3faminc_i +\beta_4motheduc_i + \beta_5fatheduc_i + u_i
\end{align*}
donde \textit{bwght} es el peso en libras, \textit{cigs} número promedio de cigarros fumados por la madre durante el embarazo, \textit{parity} orden de nacimiento del niño, \textit{faminc} ingreso anual de la familia, \textit{motheduc} y \textit{fatheduc} años de escolaridad de la madre y el padre, respectivamente.\\ \textbf{Utilizar la base de dato \textit{bwght.RData} y observar que \textit{motheduc} y \textit{fatheduc} tienen datos no disponibles}
<<>>=
load("C:/Users/81799/Downloads/bwght.RData")
@
\subsection{Estimar el modelo y explicar los resultados.}
<<>>=
bwght <- data$bwght; 
cigs <- data$cigs
parity <- data$parity
faminc <- data$faminc
motheduc <- data$motheduc
fatheduc <- data$fatheduc
datos <- data.frame(bwght, cigs, parity, faminc , motheduc , fatheduc)
ols <- lm(bwght ~cigs + parity + faminc + motheduc + fatheduc  , data = datos); ols
@
Los resultados obtenidos nos dicen que la cantidad de cigarros fumados por la madre durante el embarazo, e inclusive datos que parecieran que no influyen en el peso del niño como: años de escolaridad de la madre y padre, ingreso anual de la familia, influye en el peso.
<<>>=
linearHypothesis( ols, c(" motheduc = 0", "fatheduc = 0"),white.adjust="hc1", test = "F" )
qf(c(0.005,0.995),2,Inf) #valor critico al 99% 
qf(c(0.025,0.975),2,Inf) #valor critico al 95% 
@
\begin{align*}
H_0&: \beta _4= \beta _5=0 \\ H_a&: \beta _4\neq 0  \beta _5\neq 0
\end{align*}
Como el estadístico F está dentro de la región de aceptación, entonces se acepta la hipótesis nula hasta con un $99\%$ de confiabilidad. Es decir, podemos omitir las variables \textit{motheduc y fatheduc} en la regresión. 
\subsection{Comprobar la hipótesis del inciso anterior con errores homocedásticos y en términos de OLS restringidos y no restringidos}
<<>>=
#R^2 no restringida
R2.nr <- summary(ols)$r.squared; R2.nr
#R^2 Restringida
ols.r <- lm( bwght ~ cigs + parity + faminc , data= datos )
R2.r <- summary(ols.r)$r.squared; R2.r
F.homo <- ( (R2.nr - R2.r) / 2) / ( (1 - R2.nr) / (1388 - 4)); F.homo
qf(c(0.005, 0.995), 2, Inf) #confiabilidad al 99% 
qf(c(0.025, 0.975), 2, Inf) #confiabilidad al 95% de probabilidad
pf(F.homo, 2, Inf, lower.tail = F) #p value F
@
Al hacer las pruebas de hipótesis para un nivel de significancia de  $\alpha = 0.05$ y $\alpha = 0.01$ podemos observar que tenemos un $p-value =  0.05805638$, el cual es mayor a los dos valores de $\alpha$, por lo tanto, podemos decir que la escolaridad de los padres no influye en el peso del niño.











\section{Utilizar base de datos \textbf{Growth.RData}, excluir a Malta}
<<eval=TRUE, echo=TRUE, fig.height=3.5, fig.width=4.5, message=FALSE, warning=FALSE, comment=NA>>=
load("C:/Users/81799/Downloads/Growth.RData")
@

<<>>=
tradeshare <- Growth[Growth$country_name != "Malta" , 5 ]
growth <- Growth[Growth$country_name != "Malta" , 2 ]
yearsschool <- Growth[Growth$country_name != "Malta" , 6 ]
rev_coups <- Growth[Growth$country_name != "Malta" , 7 ]
assasinations <- Growth[Growth$country_name != "Malta" , 8 ]
rgdp60 <- Growth[Growth$country_name != "Malta" , 4 ]
sinmalta <- data.frame(tradeshare,growth,yearsschool,rev_coups,assasinations,rgdp60)
@
\subsection{Estimar regresión de: \textit{Growth} contra \textit{TradeShare} }
<<>>=
ols <- lm(growth~tradeshare , data = sinmalta); ols 
@
\subsubsection{Intervalo de confianza}
<<>>=
e.2 <- residuals(ols)^2
n <-length(e.2)
desvio.x <-(sinmalta$tradeshare-mean(sinmalta$tradeshare))^2
var.b <-(n/(n-2))*(sum(desvio.x*e.2)/(sum(desvio.x)^2))
sd.b <- sqrt(var.b)
s2.e <- sum(e.2)/(n-2) #sigma
b.var.homo <- s2.e/(sum(desvio.x))
b.sd.homo <- sqrt(b.var.homo)
cbind(inf=ols$coef[2]-1.96*b.sd.homo,sup=ols$coef[2]+1.96*b.sd.homo)
@ 
\subsubsection{¿Es estadísticamente significativo el coeficiente al 5\% ?}
<<>>=
coeftest(ols,vcov=vcovHC, type="const")
@
Como el p-value es $>.05$
<<>>=
t.homo <- ols$coef[2]/b.sd.homo
p.value.homo <- 2*pnorm(-abs (t.homo));p.value.homo
@
Entonces, \textbf{No es estadísticamente significativo}

\subsubsection{Estimar regresión de: \textit{Growth} contra \textit{yearsschool} }
<<>>=
ols <- lm(growth~yearsschool , data = sinmalta); ols
@

\subsubsection{Estimar regresión de: \textit{Growth} contra \textit{RevCoups} }
<<>>=
ols <- lm(growth~rev_coups , data = sinmalta); ols 
@

\subsubsection{Estimar regresión de: \textit{Growth} contra \textit{assasinations} }
<<>>=
ols <- lm(growth~assasinations , data = sinmalta); ols
@
\subsubsection{Estimar regresión de: \textit{Growth} contra \textit{rgdp60}}
<<>>=
ols <- lm(growth~rgdp60 , data = sinmalta); ols
@
\subsection{Comprobar si en conjunto las variable \textit{Tradeshare, YearsSchool,RevCoups y Assasinations}deben excluirse de la regresión , utilizar homocedástico y heterocedástico, de forma manual y por linea de comando}
<<>>=
ols.sm<- lm(growth ~ tradeshare+yearsschool+rev_coups+rgdp60+assasinations, data =sinmalta);ols.sm

#Regresión
R<- matrix(c(0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1),4);R
r<- matrix(c(0,0,0,0),4);r
beta <- matrix(c(ols.sm$coef[2],ols.sm$coef[3],ols.sm$coef[4],ols.sm$coef[5],ols.sm$coef[6]), 5); beta
q <- nrow(R); q
vcov.hete <- vcovHC(ols.sm, type="HC1")[2:6, 2:6]; vcov.hete
F.hete <- (t(R%*%beta-r)%*%solve(R%*%vcov.hete%*%t(R))%*%(R%*%beta-r))/q;F.hete
pf(F.hete,q,Inf,lower.tail=F) #p value F
@
\subsubsection{ Linea de comando-Heterocedástico}
<<>>=
linearHypothesis(ols.sm,c("yearsschool = 0", "rev_coups = 0", 
"assasinations= 0", "rgdp60 = 0"),white.adjust="hc1",test="F")
@
\textbf{Es estadísticamente significativo}
\subsubsection{Forma manual-homocedástico}
<<>>=
R2.nr <- summary(ols.sm)$r.squared; R2.nr
ols.r <- lm(growth ~ tradeshare , data=sinmalta) 
R2.r <- summary(ols.r)$r.squared; R2.r
F.homo <- ((R2.nr-R2.r)/(q)) / ((1-R2.nr) /(65-q)); F.homo
pf(F.homo, q, 61, lower.tail = F) #P-value
@
\subsubsection{Linea de comando -Homocedástico}
<<>>=
linearHypothesis(ols.sm, c("yearsschool = 0", "rev_coups = 0",
"assasinations= 0", "rgdp60 = 0"), test="F")
@



\subsection{ Verificar si las variables \textit{\textbf{tradeshare} } y \textit{\textbf{yearsschool} } tienen el mismo coeficiente,utilizar robustosa a la heterocedasticidad, en forma manual y comprobar por linea de comando.}

\textit{forma manual-heterocedástico}
<<>>=
ols.ty <-lm(growth ~ tradeshare+yearsschool, data =sinmalta)
linearHypothesis(ols.ty , c(" tradeshare = 0", "yearsschool= 0"),white.adjust="hc1",test="F")
@

\textiti{Forma manual-homocedástico}

<<>>=
linearHypothesis(ols.ty , c(" tradeshare = 0", "yearsschool= 0"), test="F")
@

\subsection{¿Son significativas en forma global todas las variables? Deducir el estadítico con línea de comando y comprobar a apertir de modelo restringido y no restringido.}
  
  \subsubsection{  $R^2$ del modelo no restringido:}
<<>>=
R2.nr <- summary(ols.sm)$r.squared; R2.nr
@
  \subsubsection{ Modelo restringido}
  
<<>>=
ols.R <- lm(growth ~ 1 , data=sinmalta)
R2.R <- summary(ols.R)$r.squared; R2.R; R2.R
F.su <- ((R2.nr-R2.R)/(3)) / ((1-R2.nr) /(420-4)); F.su
@  

\end{document}